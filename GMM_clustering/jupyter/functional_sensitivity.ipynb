{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we construct an influence function for the number of clusters. Then, we examine the effect of several functional perturbations to the GMM fit of the iris data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/BNP_sensitivity/venv/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bnpgmm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4845f9fcbe21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# GMM libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbnpgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm_clustering_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgmm_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbnpgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbnpgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm_optimization_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgmm_optim_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bnpgmm'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import numpy as np\n",
    "from jax import scipy as sp\n",
    "from jax import random\n",
    "\n",
    "import numpy as onp\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import paragami\n",
    "\n",
    "# GMM libraries\n",
    "import bnpgmm.gmm_clustering_lib as gmm_lib\n",
    "import bnpgmm.utils_lib as utils_lib\n",
    "import bnpgmm.gmm_optimization_lib as gmm_optim_lib\n",
    "from bnpgmm import gmm_posterior_quantities_lib\n",
    "\n",
    "# BNP libraries\n",
    "from bnpmodeling.sensitivity_lib import HyperparameterSensitivityLinearApproximation, get_cross_hess\n",
    "from bnpmodeling import result_loading_utils, influence_lib\n",
    "from bnpmodeling.bnp_optimization_lib import optimize_kl\n",
    "import bnpmodeling.functional_sensitivity_lib as func_sens_lib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and plot the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris data\n",
    "dataset_name = 'iris'\n",
    "iris_obs, iris_species = utils_lib.load_iris_data()\n",
    "dim = iris_obs.shape[1]\n",
    "n_obs = len(iris_species)\n",
    "\n",
    "iris_obs = np.array(iris_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris data consists of 150 sampled irises; each iris consists of four measurements, \n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pca and plot\n",
    "pca_fit, pc_iris_obs, colors1, colors2 = utils_lib.get_plotting_data(iris_obs)\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "fig = fig1.add_subplot(111)\n",
    "utils_lib.plot_clusters(pc_iris_obs[:, 0], pc_iris_obs[:, 1], iris_species, colors1, fig)\n",
    "fig.set_xlabel('PC1')\n",
    "fig.set_ylabel('PC2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get priors\n",
    "prior_params_dict, prior_params_paragami = gmm_lib.get_default_prior_params(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gauss-Hermite points for integrating logitnormal stick-breaking prior\n",
    "gh_deg = 8\n",
    "gh_loc, gh_weights = hermgauss(gh_deg)\n",
    "\n",
    "# convert to jax arrays\n",
    "gh_loc, gh_weights = np.array(gh_loc), np.array(gh_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the initial fit at $\\alpha = \\alpha_0$ obtained from `main_parametric_sensitivity.ipynb`. \n",
    "\n",
    "Make sure to run that notebook first, which will save the initial model that we re-use here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vb fit: make sure you've already run  the notebook `main_parametric_sensitivity.ipynb`! \n",
    "# That notebook will save `initial_fit.npz`\n",
    "vb_opt_dict, vb_params_paragami, meta_data = paragami.load_folded('./initial_fit.npz')\n",
    "vb_opt = vb_params_paragami.flatten(vb_opt_dict, free = True)\n",
    "\n",
    "\n",
    "# get initial prior parameter alpha \n",
    "alpha0 =  meta_data['alpha0'] \n",
    "prior_params_dict['dp_prior_alpha'] = alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks that the KLs match, so we've loaded the correct model\n",
    "kl = gmm_lib.get_kl(iris_obs, vb_opt_dict, prior_params_dict, gh_loc, gh_weights)\n",
    "assert np.abs(meta_data['final_kl'] - kl) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define sensitivity class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is a class that contains a method to compute and invert the Hessian. \n",
    "\n",
    "We will set the perturbation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_fun(vb_params_free, epsilon): \n",
    "\n",
    "    # NOTE! epsilon doesn't actual enter \n",
    "    # into this function. \n",
    "\n",
    "    # since the initial fit is at epsilon = 0, \n",
    "    # we just return the actual KL\n",
    "\n",
    "    # we will set the perturbation later \n",
    "\n",
    "    vb_params_dict = vb_params_paragami.fold(vb_params_free, \n",
    "                                             free = True)\n",
    "\n",
    "    return gmm_lib.get_kl(iris_obs, \n",
    "                          vb_params_dict,\n",
    "                          prior_params_dict,\n",
    "                          gh_loc,\n",
    "                          gh_weights).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb_sens = HyperparameterSensitivityLinearApproximation(objective_fun,\n",
    "                                                    vb_opt,\n",
    "                                                    hyper_par_value0 = 0., \n",
    "                                                    # the perturbed objective is null for now. \n",
    "                                                    # will set appropriately later\n",
    "                                                    hyper_par_objective_fun = lambda x, y : 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute influence function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this contains methods to compute the influence function\n",
    "influence_operator = influence_lib.InfluenceOperator(vb_opt, \n",
    "                                                     vb_params_paragami, \n",
    "                                                     vb_sens.hessian_solver,\n",
    "                                                     prior_params_dict['dp_prior_alpha'],\n",
    "                                                     stick_key = 'stick_params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the posterior statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `g` is the expected number of **in-sample** clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng_key = jax.random.PRNGKey(223)\n",
    "\n",
    "def _g(vb_free):\n",
    "    \n",
    "    vb_params_dict = vb_params_paragami.fold(vb_free, free = True)\n",
    "    \n",
    "    return gmm_posterior_quantities_lib.get_e_num_clusters_from_vb_dict(iris_obs,\n",
    "                                                    vb_params_dict,\n",
    "                                                    gh_loc, gh_weights,\n",
    "                                                    threshold = 0,\n",
    "                                                    n_samples = 10000, \n",
    "                                                    prng_key = prng_key)\n",
    "\n",
    "g = jax.jit(_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get gradient of posterior statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the gradient of g and compile\n",
    "get_grad_g = jax.jit(jax.jacobian(_g, argnums = 0))\n",
    "_ = get_grad_g(vb_opt).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the gradient of g at the optimal vb parameters\n",
    "grad_g = get_grad_g(vb_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, invert the hessian and compute the influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the grid on which we compute the influence function\n",
    "logit_v_grid = np.linspace(-8, 8, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the influence function evaluated at `logit_v_grid`\n",
    "influence_grid, _ = \\\n",
    "    influence_operator.get_influence(logit_v_grid, \n",
    "                                     grad_g, \n",
    "                                     weight_by_inv_prior = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logit_v_grid, influence_grid)\n",
    "\n",
    "# y = 0 line\n",
    "plt.plot(logit_v_grid, logit_v_grid * 0., color = 'black')\n",
    "\n",
    "plt.xlabel('Logit stick propn')\n",
    "plt.ylabel('Influence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to run experiments for different perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_derivatives(f_obj, epsilon_vec, fully_linearize = False): \n",
    "    \n",
    "    \"\"\"\n",
    "    this function returns the results from our linear approximation. \n",
    "    \n",
    "    f_obj: a class that defines the perturbation\n",
    "    epsilon_vec: is a vector of epsilons at which we evalute the perturbation. \n",
    "        epsilon = 0 is the original prior, epsilon = 1 is the alternative prior. \n",
    "    fully_linearize: a boolean; if false, we linearize only the\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # set derivative \n",
    "    vb_sens._set_cross_hess_and_solve(f_obj.hyper_par_objective_fun)\n",
    "        \n",
    "    # get lr predictions \n",
    "    lr_list = []\n",
    "    \n",
    "    if fully_linearize: \n",
    "        # this is the derivative of g with respect to the hyper parameter (epsilon)\n",
    "        dg_dhyper = np.dot(grad_g, vb_sens.dinput_dhyper) \n",
    "        lr_g_vec = dg_dhyper * epsilon_vec + g(vb_sens.opt_par_value)\n",
    "    \n",
    "    else: \n",
    "        for epsilon in epsilon_vec: \n",
    "        \n",
    "            # get linearized variational parameters at epsilon\n",
    "            lr_free_params = vb_sens.predict_opt_par_from_hyper_par(np.array([epsilon])).block_until_ready()\n",
    "            \n",
    "            lr_list.append(lr_free_params)\n",
    "        \n",
    "        # evalate g as a function of the variational parameters\n",
    "        lr_g_vec = result_loading_utils.get_post_stat_vec(g, lr_list)\n",
    "        \n",
    "    return lr_g_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refit_results(f_obj, epsilon_vec): \n",
    "    \n",
    "    # f_obj contains the perturbation \n",
    "    \"\"\"\n",
    "    \n",
    "    this function refits the variational parameters \n",
    "    after a perturbation for a range of epsilons. \n",
    "    \n",
    "    f_obj: a class that defines the perturbation\n",
    "    epsilon_vec: is a vector of epsilons at which we refit the variational parameters. \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # define objective \n",
    "    def _f(vb_free, epsilon): \n",
    "        \n",
    "        vb_params_dict = vb_params_paragami.fold(vb_free, \n",
    "                                                 free = True)\n",
    "\n",
    "        return gmm_lib.get_kl(iris_obs, \n",
    "                              vb_params_dict,\n",
    "                              prior_params_dict,\n",
    "                              gh_loc, gh_weights, \n",
    "                              e_log_phi = lambda x, y : f_obj.e_log_phi_epsilon(x, y, epsilon))\n",
    "    \n",
    "    \n",
    "    # jit functions and gradients\n",
    "    f = jax.jit(_f)\n",
    "    get_grad = jax.jit(jax.grad(_f, 0))\n",
    "    \n",
    "    def _get_hvp(x, epsilon, v):\n",
    "        return jax.jvp(jax.grad(_f, argnums = 0), (x, epsilon), (v, 0.))[1]\n",
    "\n",
    "    get_hvp = jax.jit(_get_hvp)\n",
    "    \n",
    "    # optimize\n",
    "    vb_refit_list = []\n",
    "    for epsilon in epsilon_vec: \n",
    "        print('\\n################')\n",
    "        print('re-optimzing with epsilon = ', epsilon)\n",
    "        print('################')\n",
    "\n",
    "        _, vb_opt_pert, _, _ = \\\n",
    "            optimize_kl(lambda x : f(x, epsilon),\n",
    "                         deepcopy(vb_opt_dict), \n",
    "                         vb_params_paragami, \n",
    "                         get_grad = lambda x : get_grad(x, epsilon),\n",
    "                         get_hvp = lambda x, v: get_hvp(x, epsilon, v),\n",
    "                         run_lbfgs = False,\n",
    "                         run_newton = True)                          \n",
    "        \n",
    "        vb_refit_list.append(vb_opt_pert)\n",
    "        \n",
    "    return vb_refit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(f_obj):    \n",
    "    \n",
    "    epsilon_vec = np.linspace(0, 1, 6)\n",
    "    \n",
    "    ###################\n",
    "    # get refit results\n",
    "    ###################\n",
    "    # get re-fitted vb parameters\n",
    "    vb_refit_list = get_refit_results(f_obj, epsilon_vec)\n",
    "    \n",
    "    # get posterior statistic at re-fitted vb params\n",
    "    refit_g_vec = result_loading_utils.get_post_stat_vec(g, vb_refit_list)\n",
    "\n",
    "    ###################\n",
    "    # get lr results\n",
    "    ###################\n",
    "    # posterior statistic with our linear approximation\n",
    "    lr_g_vec = get_lr_derivatives(f_obj, epsilon_vec, False)\n",
    "        \n",
    "    return refit_g_vec, lr_g_vec, epsilon_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(refit_g_vec, lr_g_vec, epsilon_vec, f_obj): \n",
    "    \n",
    "    ###################\n",
    "    # plot prior x influence function \n",
    "    ###################\n",
    "    fig, ax = plt.subplots(1, 4, figsize = (14, 3.5))\n",
    "    \n",
    "    log_phi_grid = f_obj.log_phi(logit_v_grid)\n",
    "    \n",
    "    # scale influence so it has l-inf norm 1\n",
    "    infl_norm = np.abs(influence_grid).max()\n",
    "    scale = np.abs(log_phi_grid).max() / infl_norm\n",
    "    \n",
    "    ax[0].plot(logit_v_grid, influence_grid * scale,\n",
    "               label = 'influence')\n",
    "    # plot y = 0\n",
    "    ax[0].axhline(0, color = 'black')\n",
    "\n",
    "    # overlay bump\n",
    "    ax[0].plot(logit_v_grid, \n",
    "               log_phi_grid,\n",
    "               color = 'grey', \n",
    "               label = 'perturbation')\n",
    "\n",
    "    ax[0].fill_between(logit_v_grid, \n",
    "                       0,\n",
    "                       log_phi_grid,\n",
    "                       color = 'grey',\n",
    "                       alpha = 0.5)\n",
    "    ax[0].legend(loc = 'lower left')\n",
    "    \n",
    "    ###################\n",
    "    # plot perturbed prior \n",
    "    ###################\n",
    "    # compute perturbed prior\n",
    "    prior_perturbation = func_sens_lib.PriorPerturbation(\n",
    "                                alpha0 = alpha0,\n",
    "                                log_phi = f_obj.log_phi, \n",
    "                                logit_v_ub = 10, \n",
    "                                logit_v_lb = -10)\n",
    "\n",
    "    prior_perturbation._plot_priors(ax[1])\n",
    "    prior_perturbation._plot_priors_constrained(ax[2])\n",
    "    ax[2].legend()\n",
    "\n",
    "    for j in range(3): \n",
    "        ax[j].set_xlabel('logit-sticks')\n",
    "\n",
    "    ###################\n",
    "    # plot posterior statistic \n",
    "    ###################                        \n",
    "    # plot refit results\n",
    "    ax[3].plot(epsilon_vec,\n",
    "                  refit_g_vec - refit_g_vec[0], \n",
    "                  '-o', \n",
    "                  label = 'refit')\n",
    "\n",
    "    # plot lr results\n",
    "    ax[3].plot(epsilon_vec, \n",
    "                  lr_g_vec - lr_g_vec[0], \n",
    "                  '-o', \n",
    "                  label = 'lr')\n",
    "\n",
    "    ax[3].set_xlabel('epsilon')\n",
    "    ax[3].axhline(0, color = 'black')\n",
    "\n",
    "    ax[3].legend()\n",
    "    ax[3].set_ylabel('g(pert) - g(init)')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_and_save(f_obj): \n",
    "    refit_g_vec, lr_g_vec, epsilon_vec = \\\n",
    "        run_analysis(f_obj)\n",
    "    \n",
    "    plot_results(refit_g_vec, lr_g_vec, epsilon_vec, f_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\tilde \\nu_k$ be the logit stick-breaking proportions, and let $p_0$ be the initial prior on $\\tilde \\nu_k$. \n",
    "In our case, $p_0$ is the prior such that $\\nu_k = \\text{sigmoid}(\\tilde \\nu_k)$ is distributed as $\\text{Beta}(1, \\alpha_0)$. \n",
    "\n",
    "\n",
    "We perturb $p_0$ multiplicatively, so the perturbed prior is \n",
    "\\begin{align*}\n",
    "p_\\epsilon(\\tilde \\nu_{k}) \\propto p_0(\\tilde \\nu_k) \\phi(\\tilde \\nu_k)^\\epsilon. \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first three perturbations, we let $\\phi$ be a Gaussian p.d.f. We consider three Gaussian perturbations with different means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pert(x, loc, scale): \n",
    "    return sp.stats.norm.pdf(x, loc, scale) * np.sqrt(2 * np.pi) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_phi_gauss1 = lambda x : gaussian_pert(x, loc = -4, scale = 0.75)\n",
    "\n",
    "f_obj_gauss1 = func_sens_lib.FunctionalPerturbationObjective(log_phi_gauss1, \n",
    "                                                          vb_params_paragami, \n",
    "                                                          gh_loc = gh_loc, \n",
    "                                                          gh_weights = gh_weights, \n",
    "                                                          stick_key = 'stick_params')\n",
    "\n",
    "run_all_and_save(f_obj_gauss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_phi_gauss2 = lambda x : gaussian_pert(x, loc = -2., scale = 0.75)\n",
    "\n",
    "f_obj_gauss2 = func_sens_lib.FunctionalPerturbationObjective(log_phi_gauss2, \n",
    "                                                          vb_params_paragami, \n",
    "                                                          gh_loc = gh_loc, \n",
    "                                                          gh_weights = gh_weights, \n",
    "                                                          stick_key = 'stick_params')\n",
    "\n",
    "run_all_and_save(f_obj_gauss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_phi_gauss3 = lambda x : gaussian_pert(x, loc = -0.5, scale = 0.75) # log_phi_gauss1(x) + log_phi_gauss2(x)\n",
    "\n",
    "f_obj_gauss3 = func_sens_lib.FunctionalPerturbationObjective(log_phi_gauss3, \n",
    "                                                          vb_params_paragami, \n",
    "                                                          gh_loc = gh_loc, \n",
    "                                                          gh_weights = gh_weights, \n",
    "                                                          stick_key = 'stick_params')\n",
    "\n",
    "run_all_and_save(f_obj_gauss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst-case perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute results under the worst-case perturbation with L-infinity norm = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the worst-case perturbation\n",
    "e_log_wc_phi = lambda x,y : worst_case.get_e_log_linf_perturbation(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_case = influence_lib.WorstCasePerturbation(influence_fun = None, \n",
    "                                                    logit_v_grid = logit_v_grid, \n",
    "                                                    delta = 1.,\n",
    "                                                    cached_influence_grid = influence_grid)\n",
    "\n",
    "f_obj_wc = func_sens_lib.FunctionalPerturbationObjective(worst_case.log_phi, \n",
    "                                                          vb_params_paragami, \n",
    "                                                          e_log_phi = e_log_wc_phi, \n",
    "                                                          gh_loc = gh_loc, \n",
    "                                                          gh_weights = gh_weights, \n",
    "                                                          stick_key = 'stick_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all_and_save(f_obj_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnp_sensitivity_public",
   "language": "python",
   "name": "bnp_sensitivity_public"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
